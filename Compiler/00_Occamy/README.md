# Occamy

# ìš”ì•½

- problem1 : edge device ê°™ì€ ê²½ìš° ì‘ì€ memory size ë•Œë¬¸ì— ëŒë¦´ ìˆ˜ ìˆëŠ” NNì´ ì œí•œ ëœë‹¤.
- problem2 : alloc, de-alloc ë“± memory ê´€ë ¨ opë“¤ì´ ê½¤ í° latencyë¥¼ ì°¨ì§€í•œë‹¤.

![image.png](image.png)

- memory copyëŠ” asynchronous copyë¡œ ê°€ë ¤ì§ˆ ìˆ˜ ìˆì§€ë§Œ, memory alloc/de-allocì€ synchronization primitive ë•Œë¬¸ì—
- design : occamy (3 steps)
    - Liveness-aware Memory Operation Insertion
        - Insert GPU memory operations (`malloc`, `memcpy`, `dealloc`) into DNN IR.
        - Use this data to build a **liveness table** per tensor.
        
        ![image.png](image%201.png)
        
        - í•„ìš”í•œ tensor ë¥¼ í•˜ë‚˜ í•˜ë‚˜ allocí•˜ê³  de-allocí•˜ëŠ” ë°©ì‹
        - Eager mode : ë¯¸ë¦¬ allocí•´ì„œ í• ë‹¹í•´ë‘ . (ì´ëŸ¬ë©´ ë™ timeì— ì—¬ëŸ¬ tensorë“¤ì´ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê³  ìˆì–´ì„œ footprintë¥¼ ë§ì´ ëª» ê°€ì ¸ê°)
        - Lazy memory : ê·¸ë•Œ ê·¸ë•Œ í•„ìš”í•  ë•Œ allocí•˜ê³  de-alloc í•¨. ì´ëŸ¬ë©´ OS system call, Mutexë¡œ ì¸í•œ atomic access ë•Œë¬¸ì— runtimeì´ ëŠë ¤ì§.
        - ì¦‰, eager vs. lazyëŠ” footprint-runtime trade-off ê´€ê³„ì„.
        - ë³¸ ë…¼ë¬¸ì—ì„œëŠ” liveness tableì„ ë§Œë“¤ì–´ì„œ, Aì™€ Bê°€ ì„œë¡œ ë‹¤ë¥¸ timeì— í•„ìš”í•˜ë©´, Aë¥¼ ìœ„í•´ allocí•œ ê³µê°„ì„ Bì„ ìœ„í•´ì„œë„ ì“°ëŠ” ë°©ì‹.
            - (ê¸°ì¡´) A alloc â†’ A ì‚¬ìš© â†’ A de-alloc â†’ B alloc â†’ B ì‚¬ìš© â†’ B de-alloc
            - (ì œì•ˆ) A alloc â†’ A ì‚¬ìš© â†’ B ì‚¬ìš© â†’ A de-alloc
    - Use this data to build a **liveness table** per tensor.
        - **Layer Fusion** : Merge compatible operations (e.g.)
        
        ```
        Conv + ReLU â†’ Conv-ReLU
        ```
        
        - Tensor Coalescing: Reuse memory between input and output tensors for **elementwise ops** (e.g.,)
        
        ```
        Add
        ```
        
        - avoids extra memory for temporary outputs.
        - â†’ Fewer memory allocations, more memory reuse.
    - **Memory Pool Code Generation** :Generates instructions to emulate mallocs within the pool. â†’ Eliminates need for dynamic allocation or deallocation calls at runtime.
    
    ```
    DNN.Mem-offset(base, offset, size)
    ```
    
- implementation
    - Built on **MLIR** by extending **ONNX-MLIR** (originally CPU-only).
    - Added GPU support with CUDA backend.
    - Compiles ONNX models into LLVM IR with memory pool logic.

```cpp
ONNX IR -> DNN IR -> LLVM IR -> executible binary
```

ONNX , DNN IR levelì—ì„œ liveness tableì„ ìœ ì§€í•˜ê³ , DNN IR levelì—ì„œ layer fusion/Tensor coaleascingì„ ìœ ì§€í•˜ê³ , DNN IR levelì—ì„œ memory pool ê´€ë ¨ëœ (de)allocë„ ì¼ì›í™”í•´ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ LLVM IRë¡œ ê°„ë‹¤. ë¬¼ë¡  MLIR íŠ¹ì§• ì²˜ëŸ¼ DNN IR ë‹¨ê³„ì—ì„œ ONNX IRë„ ì¼ë¶€ ì¡´ì¬í•œë‹¤. ì´í›„, `DNN.Conv` ë“±ì˜ IRì€ cuDNN kerenl callë¡œ ëŒ€ì²´í•˜ê³ , link ê±¸ì–´ì„œ GPU ë¥¼ enable í•œë‹¤. 

![image.png](image%202.png)

# implemetation tools (ë‚˜ì˜ ì¶”ì¸¡)

|  Step | Verdict | Notes |
| --- | --- | --- |
| ONNX-MLIR frontend | âœ… âœ”ï¸ | Used ONNX dialect as input |
| ONNX â†’ DNN IR | âœ… âœ”ï¸ | Via MLIR rewrite passes |
| Optimization passes | âœ… âœ”ï¸ | Custom MLIR passes for fusion & pooling |
| DNN IR â†’ LLVM IR | âœ… âœ”ï¸ | Using MLIR pattern rewrites |
| Linking with cuDNN | ğŸ”¶ Mostly right | Probably used `clang -lcudnn`, not `llc` directly |
| Executable binary | âœ… âœ”ï¸ | Result is a GPU-inference binary |

# implementation details

MLIR levelì—ì„œ ëª¨ë“  optimization ì‘ì—…ì„ í•˜ê³ , ìƒˆë¡œìš´ instructionë“¤ë„ ì§‘ì–´ ë„£ëŠ”ë‹¤. í•˜ì§€ë§Œ ë§ˆì§€ë§‰ì— LLVM IRë¡œ ê°€ì„œ ptxë¡œ code-gen í•  ìˆ˜ ìˆê²Œë” í•œë‹¤. 

```cpp
ONNX IR â†’ DNN IR (MLIR) â†’ LLVM IR â†’ CUDA Runtime API â†’ Executable
```

MLIRì—ì„œëŠ” dialect í•˜ë‚˜ë¥¼ ì¶”ê°€í–ˆë‹¤.

í•˜ë‚˜ì˜ instructionì„ ë³´ì—¬ì£¼ëŠ” êµ¬ì²´ì  ì˜ˆì‹œ

```cpp
onnx.Conv â†’ DNN.Conv â†’ LLVM call to cudnnConvolutionForward()
```

call í•˜ëŠ” ë¶€ë¶„ì´ ë°”ë¡œ ì´ ë…¼ë¬¸ì—ì„œ ì–˜ê¸°í•˜ëŠ” â€œCUDA Runtime APIâ€ì´ë‹¤. 

| CUDA Runtime API | LLVM IR Call |
| --- | --- |
| `cudaMalloc` | `call i32 @cudaMalloc(i8**, i64)` |
| `cudaMemcpy` | `call i32 @cudaMemcpy(i8*, i8*, i64, i32)` |
| `cudaFree` | `call i32 @cudaFree(i8*)` |
| `cudnnConvolutionForward` | `call i32 @cudnnConvolutionForward(i8*, ...)` |

### ğŸ› ï¸ a. **Custom GPU Memory Management Dialect (DNN IR)**

- Defined MLIR operations like:
    - `DNN.Malloc`, `DNN.Memcpy`, `DNN.Mem-offset`, `DNN.Dealloc`
    - `DNN.Conv`, `DNN.Add`, and fused variants

LLVM ìª½ì€ ë‹¤ìŒê³¼ ê°™ì€ ì—°ê²°ì„ í•´ì¤¬ë‹¤.

### ğŸ› ï¸ c. **GPU Lowering to LLVM IR with CUDA Runtime Integration**

- Lowered DNN IR into **LLVM IR**
    - Inserted calls to `cudaMalloc`, `cudaMemcpy`, and others
    - Generated PTX using LLVMâ€™s NVPTX backend
    - Linked CUDA host and kernel code for execution

ë‹¤ìŒì€ MLIR â†’ LLVM example ì´ë‹¤.

```cpp
%pool = DNN.Pool-init 2048 : memref<2048xi8>

%a = DNN.Mem-offset %pool, 0, 768 : memref<768xf32>
%b = DNN.Mem-offset %pool, 768, 540 : memref<540xf32>
%c = DNN.Mem-offset %pool, 1308, 20 : memref<20xf32>

DNN.Memcpy %a, %input_host : memref<768xf32> // H->D
DNN.Conv %a, %b, %c, %out : memref<720xf32>
```

```cpp
@cudaMalloc = declare i32 @cudaMalloc(i8** %devPtr, i64 %size)
@cudaMemcpy = declare i32 @cudaMemcpy(i8* %dst, i8* %src, i64 %count, i32 %kind)
@cudaFree = declare i32 @cudaFree(i8* %devPtr)

; Allocate unified memory pool
%pool_ptr_ptr = alloca i8*
call i32 @cudaMalloc(%pool_ptr_ptr, 2048)
%pool = load i8*, i8** %pool_ptr_ptr

; Offset pointer: %a = pool + 0
%a = getelementptr i8, i8* %pool, i64 0
%b = getelementptr i8, i8* %pool, i64 768
%c = getelementptr i8, i8* %pool, i64 1308

; H to D copy
call i32 @cudaMemcpy(i8* %a, i8* %input_host, i64 768, i32 1) ; cudaMemcpyHostToDevice

; Launch convolution kernel (assume kernel is precompiled or emitted elsewhere)
call void @conv_kernel(i8* %a, i8* %b, i8* %c, i8* %out)

```

## ğŸ”¹ 3. Linking with CUDA Runtime (Host + Device Code)

Occamy needs to:

- Emit the **LLVM IR** with CUDA runtime calls
- Compile **PTX kernels** or link **cuDNN/cuBLAS** functions
- Use `clang` or `nvcc` to compile + link

Or if using cuDNN:

```cpp
cudnnConvolutionForward(handle, ... a, b, c, out ...);

```

ì¦‰, ê°•ì˜ ë•Œ í–ˆë“¯ì´, function callì„ êµ¬í˜„í•´ì„œ cuda kernelì„ callí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§  ê²ƒì´ë‹¤. `cudnnConvolutionForward()` is a **precompiled cuDNN GPU kernel**, part of the **CUDA Deep Neural Network library.**

### ğŸ”§ Key Components Added by Occamy:

### ğŸ”¹ 1. **DNN IR with CUDA Runtime Abstractions**

Occamy defines a **new intermediate IR** (DNN IR) with GPU semantics:

- Memory ops: `DNN.Malloc`, `DNN.Memcpy`, `DNN.Dealloc`
- Pool ops: `DNN.Pool-init`, `DNN.Mem-offset`
- Compute ops: `DNN.Conv`, `DNN.Add`, possibly fused versions

These are **high-level GPU-aware operations** that will eventually become CUDA calls.

---

### ğŸ”¹ 2. **Lowering to LLVM IR with CUDA Calls**

Occamy rewrites DNN IR into **LLVM IR** using:

- **CUDA runtime API** (`cudaMalloc`, `cudaMemcpy`, `cudaFree`)
- Possibly **CUDA kernel launches** (for ops like `Conv`, `Relu`, etc.)

This is done through **custom MLIR to LLVM IR conversion passes**, which:

- Replace `DNN.Malloc(...)` â†’ `llvm.call @cudaMalloc(...)`
- Replace `DNN.Memcpy(...)` â†’ `llvm.call @cudaMemcpy(...)`
- Replace `DNN.Conv(...)` â†’ CUDA kernel launch or a cuDNN call

ğŸ“Œ This mirrors what MLIR does with CPU code (e.g., lowering `memref.alloc` â†’ `malloc`), but **for CUDA**.

Lower entire IR into LLVM IR so that standard LLVM passes (NVPTX codegen, inlining, etc.) can kick in

### âœ… Step 3: Building the Executable for CUDA

After generating LLVM IR with CUDA function calls:

- Occamy invokes **LLVM's NVPTX backend** (via `llc`) to emit **PTX**
- PTX is linked with CUDA host code (using `nvcc` or `clang --cuda`)
- The result is an **executable binary** or a **shared object** to run inference

---

### âœ… Step 4: Memory Pool Management in CUDA

Instead of using many `cudaMalloc`/`cudaFree`, which are expensive:

- Occamy generates **one `cudaMalloc`** at the start:
    
    â†’ `DNN.Pool-init(size)` â†’ `cudaMalloc(&pool, size)`
    
- All tensors are mapped to **offsets in this pool**
    
    â†’ `DNN.Mem-offset(pool, offset, size)` is treated as a pointer arithmetic: `pool + offset`
    
- No more malloc/free during execution
    
    â†’ Reduces runtime overhead and memory fragmentation
    

# ì˜ë¬¸ì 

## ì–´ë–»ê²Œ CPU only ì¸ ONNX-MLIRë¡œ GPU kernelì„ ë§Œë“¤ì—ˆì„ê¹Œ?

1. ONNX-MLIRì´ front-end ë¡œì¨ CPU only ì§€ë§Œ, MLIRì˜ GPU related MLIR(`vector`, `linalg`, or `gpu`)ë¡œ lowering í–ˆì„ ìˆ˜ë„ ìˆë‹¤. (ì¶”ì¸¡, ì•„ë‹˜.)
2. cuda kernel callì„ í–ˆê³ , í•´ë‹¹ kernelë“¤ì€ ì´ë¯¸ SIMDê°€ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í–ˆì„ ê²ƒì´ë‹¤. (ë¶„ëª…í•¨)
